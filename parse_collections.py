#!/usr/bin/env python3
"""
Automatic Gift Collections Parser
Fetches gift collections from portal-market.com API and generates mapping files
"""

import json
import os
import hashlib
from datetime import datetime, timezone
from typing import Dict, List, Tuple, Optional
from curl_cffi import requests


API_URL = "https://portal-market.com/api/collections?limit=999"
COLLECTIONS_IDS_PATH = "Portals/collections_ids.py"
COLLECTIONS_NAMES_PATH = "collections_names.py"
LOG_FILE_PATH = "parser_log.txt"
MAX_LOG_ENTRIES = 100  # Maximum number of log entries to keep

HEADERS_MAIN = {
        "Authorization": "",
        "Origin": "https://portal-market.com",
        "Referer": "https://portal-market.com/",
    }


def rotate_log() -> None:
    """Keep only the last MAX_LOG_ENTRIES entries in the log file"""
    if not os.path.exists(LOG_FILE_PATH):
        return
    
    try:
        with open(LOG_FILE_PATH, "r", encoding="utf-8") as f:
            lines = f.readlines()
        
        # Keep only the last MAX_LOG_ENTRIES
        if len(lines) > MAX_LOG_ENTRIES:
            lines = lines[-MAX_LOG_ENTRIES:]
            
            with open(LOG_FILE_PATH, "w", encoding="utf-8") as f:
                f.writelines(lines)
    except Exception as e:
        print(f"Warning: Could not rotate log file: {e}")


def log_message(message: str, level: str = "INFO") -> None:
    """Add a message to the log file"""
    timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")
    log_entry = f"[{timestamp}] [{level}] {message}\n"
    
    with open(LOG_FILE_PATH, "a", encoding="utf-8") as f:
        f.write(log_entry)
    
    print(log_entry.strip())
    
    # Rotate log after each write to prevent unlimited growth
    rotate_log()


def fetch_collections() -> Optional[Dict]:
    """Fetch collections from API with validation"""
    try:
        log_message("Fetching collections from API...")
        response = requests.get(API_URL, timeout=30, headers=HEADERS_MAIN, impersonate="chrome")
        
        # Check HTTP status
        if response.status_code != 200:
            log_message(
                f"Failed to fetch data: HTTP {response.status_code}",
                "ERROR"
            )
            return None
        
        # Parse JSON
        try:
            data = response.json()
        except json.JSONDecodeError as e:
            log_message(f"Invalid JSON response: {e}", "ERROR")
            return None
        
        # Validate structure
        if not isinstance(data, dict) or "collections" not in data:
            log_message(
                "Invalid JSON structure: missing 'collections' key",
                "ERROR"
            )
            return None
        
        if not isinstance(data["collections"], list):
            log_message(
                "Invalid JSON structure: 'collections' is not a list",
                "ERROR"
            )
            return None
        
        # Validate collection items
        for idx, collection in enumerate(data["collections"]):
            required_fields = ["id", "name", "short_name"]
            for field in required_fields:
                if field not in collection:
                    log_message(
                        f"Invalid collection at index {idx}: missing '{field}'",
                        "ERROR"
                    )
                    return None
        
        log_message(f"Successfully fetched {len(data['collections'])} collections")
        return data
        
    except Exception as e:
        log_message(f"Exception while fetching data: {e}", "ERROR")
        return None


def generate_collections_ids(collections: List[Dict]) -> str:
    """Generate collections_ids.py content"""
    full_name_to_id = {}
    short_name_to_id = {}
    
    for collection in collections:
        collection_id = collection["id"]
        full_name = collection["name"]
        short_name = collection["short_name"]
        
        full_name_to_id[full_name] = collection_id
        short_name_to_id[short_name] = collection_id
    
    # Generate Python file content
    content = '"""\nGift Collections IDs Mapping\nAuto-generated by parse_collections.py\n"""\n\n'
    
    # FULL_NAME_TO_ID dictionary
    content += "# Full name to collection ID mapping\n"
    content += "FULL_NAME_TO_ID = {\n"
    for name in sorted(full_name_to_id.keys()):
        content += f'    "{name}": "{full_name_to_id[name]}",\n'
    content += "}\n\n"
    
    # SHORT_NAME_TO_ID dictionary
    content += "# Short name to collection ID mapping\n"
    content += "SHORT_NAME_TO_ID = {\n"
    for name in sorted(short_name_to_id.keys()):
        content += f'    "{name}": "{short_name_to_id[name]}",\n'
    content += "}\n"
    
    return content


def generate_collections_names(collections: List[Dict]) -> str:
    """Generate collections_names.py content"""
    short_to_long = {}
    long_to_short = {}
    
    for collection in collections:
        full_name = collection["name"]
        short_name = collection["short_name"]
        
        short_to_long[short_name] = full_name
        long_to_short[full_name] = short_name
    
    # Generate Python file content
    content = '"""\nGift Collections Names Mapping\nAuto-generated by parse_collections.py\n"""\n\n'
    
    # SHORT_TO_LONG_NAME dictionary
    content += "# Short name to long name mapping\n"
    content += "SHORT_TO_LONG_NAME = {\n"
    for name in sorted(short_to_long.keys()):
        content += f'    "{name}": "{short_to_long[name]}",\n'
    content += "}\n\n"
    
    # LONG_TO_SHORT_NAME dictionary
    content += "# Long name to short name mapping\n"
    content += "LONG_TO_SHORT_NAME = {\n"
    for name in sorted(long_to_short.keys()):
        content += f'    "{name}": "{long_to_short[name]}",\n'
    content += "}\n"
    
    return content


def get_file_hash(filepath: str) -> Optional[str]:
    """Get MD5 hash of file content"""
    if not os.path.exists(filepath):
        return None
    
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            content = f.read()
        return hashlib.md5(content.encode()).hexdigest()
    except Exception as e:
        log_message(f"Error reading file {filepath}: {e}", "WARNING")
        return None


def write_file_if_changed(filepath: str, content: str) -> bool:
    """Write file only if content has changed. Returns True if file was written."""
    # Ensure directory exists
    os.makedirs(os.path.dirname(filepath) if os.path.dirname(filepath) else ".", exist_ok=True)
    
    # Get hash of existing file
    old_hash = get_file_hash(filepath)
    new_hash = hashlib.md5(content.encode()).hexdigest()
    
    # Compare hashes
    if old_hash == new_hash:
        log_message(f"No changes detected in {filepath}")
        return False
    
    # Write file
    try:
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(content)
        log_message(f"File updated: {filepath}")
        return True
    except Exception as e:
        log_message(f"Error writing file {filepath}: {e}", "ERROR")
        return False


def main():
    """Main function"""
    log_message("=" * 60)
    log_message("Starting gift collections parser")
    
    # Fetch data
    data = fetch_collections()
    if data is None:
        log_message("Parser failed: could not fetch data", "ERROR")
        return 1
    
    collections = data["collections"]
    
    # Generate files
    collections_ids_content = generate_collections_ids(collections)
    collections_names_content = generate_collections_names(collections)
    
    # Write files if changed
    ids_changed = write_file_if_changed(COLLECTIONS_IDS_PATH, collections_ids_content)
    names_changed = write_file_if_changed(COLLECTIONS_NAMES_PATH, collections_names_content)
    
    # Summary
    if ids_changed or names_changed:
        log_message("Parser completed successfully: files updated")
    else:
        log_message("Parser completed successfully: no changes detected")
    
    log_message("=" * 60)
    return 0


if __name__ == "__main__":
    exit(main())
